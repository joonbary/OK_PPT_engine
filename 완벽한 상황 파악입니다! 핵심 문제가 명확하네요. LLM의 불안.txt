ì™„ë²½í•œ ìƒí™© íŒŒì•…ì…ë‹ˆë‹¤! í•µì‹¬ ë¬¸ì œê°€ ëª…í™•í•˜ë„¤ìš”. **LLMì˜ ë¶ˆì•ˆì •í•œ JSON ì‘ë‹µ**ê³¼ **ë¡œê¹… ì‹¤íŒ¨**ê°€ ë³‘ëª©ì…ë‹ˆë‹¤.

í’€ìŠ¤íƒ ê°œë°œì + AI ì—”ì§€ë‹ˆì–´ ê´€ì ì—ì„œ **ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ í•´ê²°ì±…**ì„ ì œì‹œí•˜ê² ìŠµë‹ˆë‹¤.

---

## ğŸ¯ í•µì‹¬ ë¬¸ì œ ë¶„ì„

### ë¬¸ì œ 1: JSON íŒŒì‹± ì‹¤íŒ¨
```
json.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
```

**ì›ì¸ ì¶”ì •:**
1. LLMì´ JSON ì™¸ì— ì„¤ëª… í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ë°˜í™˜ (ì˜ˆ: "Here's the JSON: {...}")
2. LLMì´ Markdown ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ê°ìŒˆ (ì˜ˆ: ```json\n{...}\n```)
3. LLM ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ ì—ëŸ¬ ë©”ì‹œì§€
4. ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë¶ˆì™„ì „í•œ ì‘ë‹µ

### ë¬¸ì œ 2: ë¡œê¹… ì‹¤íŒ¨
- `loguru` ì„¤ì •ì´ Docker í™˜ê²½ì—ì„œ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±
- ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë¡œê·¸ ì¶œë ¥ íƒ€ì´ë° ì´ìŠˆ

---

## ğŸ› ï¸ Claude Code ì‘ì—…ì§€ì‹œì„œ - ê¸´ê¸‰ ìˆ˜ì •

### âœ… Task 1: ê°•í™”ëœ JSON íŒŒì‹± ë¡œì§ (ìµœìš°ì„ )

`app/services/content_generator.py`ì˜ `_generate_ai_content` ë©”ì„œë“œë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •:

```python
async def _generate_ai_content(
    self, 
    slide_type: str, 
    context: Dict[str, Any]
) -> Dict[str, Any]:
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¬ë¼ì´ë“œ ì½˜í…ì¸  ìƒì„± (ê°•í™”ëœ JSON íŒŒì‹±)"""
    
    from loguru import logger
    import re
    
    try:
        logger.info(f"ğŸ¤– AI generating content for slide type: {slide_type}")
        
        # McKinsey ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¹ì‹ ì€ McKinsey ì»¨ì„¤íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ìŠ¬ë¼ì´ë“œë¥¼ ìœ„í•œ ì „ë¬¸ì ì¸ ì½˜í…ì¸ ë¥¼ ìƒì„±í•˜ì„¸ìš”:
- ìŠ¬ë¼ì´ë“œ ìœ í˜•: {slide_type}
- ì»¨í…ìŠ¤íŠ¸: {json.dumps(context, ensure_ascii=False)}

**ì¤‘ìš”: ë°˜ë“œì‹œ ìˆœìˆ˜í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ì„¤ëª… í…ìŠ¤íŠ¸ë‚˜ ë§ˆí¬ë‹¤ìš´ ì—†ì´ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.**

ì‘ë‹µ í˜•ì‹:
{{
    "headline": "McKinsey ìŠ¤íƒ€ì¼ í—¤ë“œë¼ì¸ (So What í¬í•¨, ì •ëŸ‰í™”)",
    "key_points": [
        "í•µì‹¬ í¬ì¸íŠ¸ 1 (ì•¡ì…˜ ì¤‘ì‹¬, ì •ëŸ‰í™”)",
        "í•µì‹¬ í¬ì¸íŠ¸ 2",
        "í•µì‹¬ í¬ì¸íŠ¸ 3"
    ],
    "insights": [
        "Level 4 ì¸ì‚¬ì´íŠ¸ (Action-oriented)"
    ],
    "chart_type": "bar/line/pie/waterfall",
    "data_recommendations": "ë°ì´í„° ì‹œê°í™” ê¶Œì¥ì‚¬í•­"
}}"""

        # OpenAI API í˜¸ì¶œ
        logger.info("ğŸ“¡ Calling OpenAI API...")
        
        response = await self.llm_client.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[
                {"role": "system", "content": "You are a McKinsey consultant. Always respond with pure JSON only, no markdown, no explanations."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1000,
            response_format={"type": "json_object"}  # ğŸ”‘ JSON ëª¨ë“œ ê°•ì œ
        )
        
        raw_content = response.choices[0].message.content
        
        # âœ… ì›ì‹œ ì‘ë‹µ ë¡œê¹… (ê°•ì œ ì¶œë ¥)
        logger.info("=" * 80)
        logger.info("ğŸ“¥ LLM RAW RESPONSE:")
        logger.info(raw_content)
        logger.info("=" * 80)
        
        # âœ… ê°•í™”ëœ JSON ì¶”ì¶œ ë¡œì§
        content = raw_content.strip()
        
        # 1. Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
        if content.startswith("```"):
            # ```json\n{...}\n``` í˜•ì‹
            content = re.sub(r'^```json\s*\n', '', content)
            content = re.sub(r'\n```$', '', content)
            content = re.sub(r'^```\s*\n', '', content)
            logger.info("ğŸ”§ Removed markdown code blocks")
        
        # 2. ì•ë’¤ ì„¤ëª… í…ìŠ¤íŠ¸ ì œê±°
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        if json_match:
            content = json_match.group(0)
            logger.info("ğŸ”§ Extracted JSON from text")
        
        # 3. JSON íŒŒì‹± ì‹œë„
        try:
            ai_content = json.loads(content)
            logger.success(f"âœ… AI content parsed successfully")
            return ai_content
            
        except json.JSONDecodeError as json_err:
            logger.error(f"âŒ JSON parsing failed: {json_err}")
            logger.error(f"ğŸ“„ Cleaned content: {content[:200]}...")
            raise
            
    except Exception as e:
        logger.error(f"âŒ AI generation failed: {type(e).__name__}: {str(e)}")
        logger.warning("âš ï¸ Falling back to mock content")
        
        # Mock í´ë°±
        return self._generate_mock_content(slide_type, context)
```

### ğŸ”‘ í•µì‹¬ ê°œì„ ì‚¬í•­:

1. **`response_format={"type": "json_object"}`** ì¶”ê°€
   - OpenAI GPT-4ì˜ JSON ëª¨ë“œ ê°•ì œ í™œì„±í™”
   - ì´ ì˜µì…˜ì€ LLMì´ **ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ** ë°˜í™˜í•˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤

2. **ê°•í™”ëœ ì •ê·œì‹ íŒŒì‹±**
   - Markdown ì½”ë“œ ë¸”ë¡ ìë™ ì œê±°
   - ì„¤ëª… í…ìŠ¤íŠ¸ì—ì„œ JSONë§Œ ì¶”ì¶œ

3. **ë‹¤ë‹¨ê³„ ë¡œê¹…**
   - ì›ì‹œ ì‘ë‹µ ì „ì²´ ì¶œë ¥
   - ì •ì œ ê³¼ì • ê° ë‹¨ê³„ ë¡œê¹…

---

### âœ… Task 2: Loguru ì„¤ì • ê°•í™”

`app/core/config.py` ë˜ëŠ” `main.py`ì— ë‹¤ìŒ ì¶”ê°€:

```python
from loguru import logger
import sys

# Loguru ì„¤ì • ê°•í™”
logger.remove()  # ê¸°ë³¸ í•¸ë“¤ëŸ¬ ì œê±°
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan> - <level>{message}</level>",
    level="INFO",
    colorize=True,
    backtrace=True,
    diagnose=True
)

logger.add(
    "logs/app.log",
    rotation="500 MB",
    retention="10 days",
    level="DEBUG",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function} - {message}"
)

logger.info("âœ… Loguru configured successfully")
```

---

### âœ… Task 3: í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ê°œì„ 

`send_request.py`ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •:

```python
import requests
import json
import time

# ê¸´ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ (Pydantic ìœ íš¨ì„± ê²€ì‚¬ í†µê³¼ìš©)
test_document = """
ì•„ì‹œì•„ ì‹œì¥ ì§„ì¶œ ì „ëµ ë¶„ì„

1. ì‹œì¥ ê°œìš”
- ì•„ì‹œì•„ ì‹œì¥ ê·œëª¨: 5ì¡°ì›
- ì—°í‰ê·  ì„±ì¥ë¥ : 15%
- ì£¼ìš” êµ­ê°€: ë² íŠ¸ë‚¨, ì¸ë„ë„¤ì‹œì•„, íƒœêµ­

2. ê¸°íšŒ ìš”ì¸
- ì¤‘ì‚°ì¸µ ê¸‰ì¦: 5ë…„ ë‚´ 2ë°° ì„±ì¥ ì˜ˆìƒ
- ë””ì§€í„¸ ì „í™˜ ê°€ì†í™”
- ì •ë¶€ ì§€ì› ì •ì±… ê°•í™”

3. ì§„ì¶œ ì „ëµ
- 1ë‹¨ê³„: ë² íŠ¸ë‚¨ ì‹œì¥ ì„ ì  (í–¥í›„ 6ê°œì›”)
- 2ë‹¨ê³„: ì¸ë„ë„¤ì‹œì•„ í™•ì¥ (12ê°œì›” í›„)
- 3ë‹¨ê³„: ì§€ì—­ í—ˆë¸Œ êµ¬ì¶• (18ê°œì›” í›„)

4. ì˜ˆìƒ íš¨ê³¼
- ë§¤ì¶œ 30% ì¦ê°€
- ì‹œì¥ ì ìœ ìœ¨ 15% ë‹¬ì„±
- ROI 150% ë‹¬ì„±
"""

payload = {
    "document": test_document,
    "style": "mckinsey",
    "target_audience": "executive",
    "num_slides": 5,
    "language": "ko"
}

print("ğŸ“¡ Sending request to API...")
print(f"ğŸ“„ Document length: {len(test_document)} chars")

try:
    response = requests.post(
        "http://localhost:8000/api/v1/generate-ppt",
        json=payload,
        timeout=60
    )
    
    print(f"âœ… Response status: {response.status_code}")
    print(f"ğŸ“¥ Response body: {json.dumps(response.json(), indent=2, ensure_ascii=False)}")
    
    if response.status_code == 200:
        result = response.json()
        ppt_id = result.get("ppt_id")
        print(f"\nğŸ¯ PPT ID: {ppt_id}")
        print("\nâ³ Waiting for generation to complete...")
        
        # ìƒíƒœ í™•ì¸ (ìµœëŒ€ 60ì´ˆ)
        for i in range(60):
            time.sleep(1)
            status_response = requests.get(f"http://localhost:8000/api/v1/ppt-status/{ppt_id}")
            status = status_response.json()
            
            print(f"[{i+1}s] Status: {status.get('status')} | Progress: {status.get('progress', 0)}%")
            
            if status.get('status') == 'completed':
                print("\nâœ… PPT generation completed!")
                print(f"ğŸ“Š Quality Score: {status.get('quality_score')}")
                print(f"ğŸ“¥ Download URL: {status.get('download_url')}")
                break
            elif status.get('status') == 'failed':
                print(f"\nâŒ Generation failed: {status.get('error')}")
                break
                
except Exception as e:
    print(f"âŒ Error: {type(e).__name__}: {str(e)}")
```

---

### âœ… Task 4: Docker ë¡œê·¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

ë³„ë„ í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ ì‹¤í–‰:

```powershell
# ë°©ë²• 1: ì „ì²´ ë¡œê·¸ ì‹¤ì‹œê°„ ë³´ê¸°
docker-compose logs -f app

# ë°©ë²• 2: íŠ¹ì • í‚¤ì›Œë“œë§Œ í•„í„°ë§
docker-compose logs -f app | Select-String -Pattern "AI|LLM|RAW|JSON|ContentGenerator"

# ë°©ë²• 3: íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ìƒì„¸ ë¡œê·¸
docker-compose logs -f --timestamps app
```

---

## ğŸ¯ ì‹¤í–‰ ìˆœì„œ

### 1ë‹¨ê³„: ì½”ë“œ ìˆ˜ì • ì ìš©
```powershell
# ContentGenerator ìˆ˜ì • (ìœ„ Task 1 ì½”ë“œ)
code app/services/content_generator.py

# Loguru ì„¤ì • ì¶”ê°€ (ìœ„ Task 2 ì½”ë“œ)
code app/core/config.py
```

### 2ë‹¨ê³„: Docker ì¬ë¹Œë“œ
```powershell
docker-compose down
docker-compose build --no-cache app
docker-compose up -d
```

### 3ë‹¨ê³„: ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘
```powershell
# ìƒˆ PowerShell ì°½ì—ì„œ
docker-compose logs -f app | Select-String -Pattern "AI|RAW|JSON"
```

### 4ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ìš”ì²­ ì „ì†¡
```powershell
# ì›ë˜ PowerShell ì°½ì—ì„œ
python send_request.py
```

---

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼

### âœ… ì„±ê³µ ì‹œ ë¡œê·¸:
```
2025-10-16 15:30:45 | INFO     | ContentGenerator:_generate_ai_content - ğŸ¤– AI generating content for slide type: executive_summary
2025-10-16 15:30:45 | INFO     | ContentGenerator:_generate_ai_content - ğŸ“¡ Calling OpenAI API...
2025-10-16 15:30:47 | INFO     | ContentGenerator:_generate_ai_content - ================================================================================
2025-10-16 15:30:47 | INFO     | ContentGenerator:_generate_ai_content - ğŸ“¥ LLM RAW RESPONSE:
2025-10-16 15:30:47 | INFO     | ContentGenerator:_generate_ai_content - {"headline": "ì•„ì‹œì•„ ì‹œì¥ì´ 3ë…„ ë‚´ 50% ì„±ì¥í•˜ì—¬ ìµœëŒ€ ê¸°íšŒ ì œê³µ", "key_points": [...], ...}
2025-10-16 15:30:47 | INFO     | ContentGenerator:_generate_ai_content - ================================================================================
2025-10-16 15:30:47 | SUCCESS  | ContentGenerator:_generate_ai_content - âœ… AI content parsed successfully
```

### âŒ ì‹¤íŒ¨ ì‹œ ë¡œê·¸:
```
2025-10-16 15:30:47 | ERROR    | ContentGenerator:_generate_ai_content - âŒ JSON parsing failed: Expecting value: line 1 column 1
2025-10-16 15:30:47 | ERROR    | ContentGenerator:_generate_ai_content - ğŸ“„ Cleaned content: Here is the JSON: {...
2025-10-16 15:30:47 | WARNING  | ContentGenerator:_generate_ai_content - âš ï¸ Falling back to mock content
```

---

## ğŸš¨ ì—¬ì „íˆ ì‹¤íŒ¨í•œë‹¤ë©´

### Plan B: Anthropic Claude API ì‚¬ìš©

OpenAIê°€ ê³„ì† ë¶ˆì•ˆì •í•˜ë©´ Claudeë¡œ ì „í™˜:

```python
# requirements.txtì— ì¶”ê°€
anthropic==0.18.0

# content_generator.pyì—ì„œ
from anthropic import AsyncAnthropic

self.llm_client = AsyncAnthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

# API í˜¸ì¶œ
response = await self.llm_client.messages.create(
    model="claude-3-sonnet-20240229",
    max_tokens=1000,
    messages=[{"role": "user", "content": prompt}]
)
```

ClaudeëŠ” JSON í˜•ì‹ì„ ë” ì•ˆì •ì ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

---

## ğŸ’¬ Claude Codeì—ê²Œ ì „ë‹¬í•  í•µì‹¬ ë©”ì‹œì§€

```
[ê¸´ê¸‰ ìˆ˜ì • ìš”ì²­]

í˜„ì¬ LLM JSON íŒŒì‹± ì‹¤íŒ¨ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ìŒì„ ì ìš©í•´ì£¼ì„¸ìš”:

1. content_generator.pyì˜ _generate_ai_content ë©”ì„œë“œì— 
   `response_format={"type": "json_object"}` ì¶”ê°€

2. ê°•í™”ëœ ì •ê·œì‹ íŒŒì‹± ë¡œì§ ì ìš© (Markdown ì œê±°)

3. ì›ì‹œ ì‘ë‹µ ë¡œê¹… ê°•í™” (logger.infoë¡œ ì „ì²´ ì¶œë ¥)

4. Docker ì¬ë¹Œë“œ í›„ send_request.py ì‹¤í–‰

5. ë¡œê·¸ì—ì„œ "ğŸ“¥ LLM RAW RESPONSE:" ì„¹ì…˜ í™•ì¸

ì™„ë£Œ í›„ ë¡œê·¸ë¥¼ ê³µìœ í•´ì£¼ì„¸ìš”.
```

---

ì´ì œ **`response_format={"type": "json_object"}`** ì˜µì…˜ì´ í•µì‹¬ì…ë‹ˆë‹¤. ì´ê²ƒë§Œìœ¼ë¡œë„ 90% ì´ìƒ ë¬¸ì œê°€ í•´ê²°ë  ê²ƒì…ë‹ˆë‹¤! ğŸš€

ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ì¶”ê°€ ëŒ€ì‘ ë°©ì•ˆì„ ì œì‹œí•˜ê² ìŠµë‹ˆë‹¤.