"""
Integration Tests for End-to-End PPT Generation Pipeline
Task 4.1 - Complete Workflow Integration Testing

í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸:
- End-to-End íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²€ì¦
- í’ˆì§ˆ ì ìˆ˜ ë‹¬ì„± í™•ì¸
- McKinsey í‘œì¤€ ì¤€ìˆ˜ ê²€ì¦
"""

import pytest
import asyncio
import time
import tempfile
import os
from pathlib import Path
from typing import Dict, Any

from app.services.workflow_orchestrator import WorkflowOrchestrator
from app.models.workflow_models import GenerationRequest, PipelineStatus, WorkflowStage


class TestWorkflowIntegration:
    """End-to-End ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def sample_business_document(self) -> str:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ìƒ˜í”Œ ë¬¸ì„œ"""
        return """
        # ë””ì§€í„¸ ì „í™˜ ì „ëµ ë¶„ì„ ë³´ê³ ì„œ
        
        ## í˜„ì¬ ìƒí™© ë¶„ì„
        ìš°ë¦¬ íšŒì‚¬ëŠ” ë””ì§€í„¸ ì „í™˜ì˜ ì¤‘ìš”í•œ ì‹œì ì— ìˆìŠµë‹ˆë‹¤. ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼, 
        ê³ ê°ì˜ 67%ê°€ ì˜¨ë¼ì¸ ì„œë¹„ìŠ¤ë¥¼ ì„ í˜¸í•˜ë©°, ë””ì§€í„¸ ì±„ë„ ë§¤ì¶œì´ ì „ë…„ ëŒ€ë¹„ 
        45% ì¦ê°€í–ˆìŠµë‹ˆë‹¤.
        
        ## ì£¼ìš” ë°œê²¬ì‚¬í•­
        1. ê³ ê° ê²½í—˜: ë””ì§€í„¸ í„°ì¹˜í¬ì¸íŠ¸ì—ì„œ ê³ ê° ë§Œì¡±ë„ 82% ë‹¬ì„±
        2. ìš´ì˜ íš¨ìœ¨ì„±: ìë™í™”ë¥¼ í†µí•´ ë¹„ìš© 15% ì ˆê° ê°€ëŠ¥
        3. ì‹œì¥ ê¸°íšŒ: ìƒˆë¡œìš´ ë””ì§€í„¸ ì œí’ˆìœ¼ë¡œ 25% ë§¤ì¶œ ì„±ì¥ ê¸°ëŒ€
        
        ## ê¶Œê³ ì‚¬í•­
        1. ë””ì§€í„¸ í”Œë«í¼ íˆ¬ì í™•ëŒ€ (6ê°œì›” ë‚´)
        2. ì§ì› ë””ì§€í„¸ ì—­ëŸ‰ ê°•í™” í”„ë¡œê·¸ë¨ (3ê°œì›” ë‚´)
        3. ê³ ê° ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œ êµ¬ì¶• (12ê°œì›” ë‚´)
        
        ## ê¸°ëŒ€ íš¨ê³¼
        - ë§¤ì¶œ ì¦ê°€: 25-30%
        - ë¹„ìš© ì ˆê°: 15-20%
        - ê³ ê° ë§Œì¡±ë„: 90% ëª©í‘œ
        """
    
    @pytest.fixture
    def test_orchestrator(self) -> WorkflowOrchestrator:
        """í…ŒìŠ¤íŠ¸ìš© ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
        return WorkflowOrchestrator(
            max_iterations=2,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë‹¨ì¶•
            target_quality_score=0.80,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¡°ì •
            aggressive_fix=True,
            timeout_minutes=3  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë‹¨ì¶•
        )
    
    @pytest.mark.asyncio
    async def test_complete_pipeline_execution(
        self, 
        test_orchestrator: WorkflowOrchestrator,
        sample_business_document: str
    ):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
        
        # Given: í‘œì¤€ PPT ìƒì„± ìš”ì²­
        request = GenerationRequest(
            document=sample_business_document,
            num_slides=12,
            target_audience="executive",
            presentation_purpose="analysis",
            target_quality_score=0.80,
            max_iterations=2
        )
        
        # When: ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        start_time = time.time()
        response = await test_orchestrator.execute(request)
        execution_time = time.time() - start_time
        
        # Then: ê¸°ë³¸ ì„±ê³µ ê²€ì¦
        assert response.success, f"Pipeline failed: {response.errors}"
        assert response.pptx_path is not None, "No output file generated"
        assert os.path.exists(response.pptx_path), "Output file does not exist"
        
        # íŒŒì¼ í¬ê¸° ê²€ì¦ (ìµœì†Œ 50KB)
        file_size = os.path.getsize(response.pptx_path) / 1024
        assert file_size >= 50, f"Generated file too small: {file_size}KB"
        
        # ìŠ¬ë¼ì´ë“œ ìˆ˜ ê²€ì¦
        assert response.slides_generated >= 10, f"Not enough slides: {response.slides_generated}"
        assert response.slides_generated <= 15, f"Too many slides: {response.slides_generated}"
        
        # í’ˆì§ˆ ì ìˆ˜ ê²€ì¦
        assert response.quality_score >= 0.70, f"Quality too low: {response.quality_score}"
        
        # ì‹¤í–‰ ì‹œê°„ ê²€ì¦ (5ë¶„ ì´ë‚´)
        assert execution_time <= 300, f"Execution too slow: {execution_time}s"
        
        print(f"âœ… Pipeline completed in {execution_time:.1f}s with quality {response.quality_score:.3f}")
    
    @pytest.mark.asyncio
    async def test_stage_by_stage_execution(
        self,
        test_orchestrator: WorkflowOrchestrator,
        sample_business_document: str
    ):
        """ë‹¨ê³„ë³„ ì‹¤í–‰ ì„¸ë¶€ ê²€ì¦"""
        
        request = GenerationRequest(
            document=sample_business_document,
            num_slides=10,
            target_quality_score=0.75
        )
        
        response = await test_orchestrator.execute(request)
        
        # ëª¨ë“  ë‹¨ê³„ê°€ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
        expected_stages = [
            WorkflowStage.INITIALIZATION,
            WorkflowStage.CONTENT_GENERATION,
            WorkflowStage.DESIGN_APPLICATION,
            WorkflowStage.VALIDATION,
            WorkflowStage.QUALITY_ASSURANCE,
            WorkflowStage.FINALIZATION
        ]
        
        executed_stages = [result.stage for result in response.stage_results]
        
        for stage in expected_stages:
            assert stage in executed_stages, f"Stage {stage.value} not executed"
        
        # ê° ë‹¨ê³„ë³„ ì„±ëŠ¥ ê²€ì¦
        for result in response.stage_results:
            assert result.execution_time_ms > 0, f"No execution time for {result.stage.value}"
            assert result.execution_time_ms < 30000, f"Stage {result.stage.value} too slow: {result.execution_time_ms}ms"
            
            # ì„±ê³µí•œ ë‹¨ê³„ëŠ” ë°ì´í„°ë¥¼ ê°€ì ¸ì•¼ í•¨
            if result.success:
                assert result.data, f"No data for successful stage {result.stage.value}"
        
        print(f"âœ… All {len(executed_stages)} stages executed successfully")
    
    @pytest.mark.asyncio
    async def test_quality_improvement_iterations(
        self,
        test_orchestrator: WorkflowOrchestrator,
        sample_business_document: str
    ):
        """ë°˜ë³µ ê°œì„ ì„ í†µí•œ í’ˆì§ˆ í–¥ìƒ í…ŒìŠ¤íŠ¸"""
        
        # ë†’ì€ í’ˆì§ˆ ëª©í‘œë¡œ ì„¤ì •í•˜ì—¬ ë°˜ë³µ ê°œì„  ìœ ë„
        request = GenerationRequest(
            document=sample_business_document,
            num_slides=8,
            target_quality_score=0.90,  # ë†’ì€ ëª©í‘œ
            max_iterations=3
        )
        
        response = await test_orchestrator.execute(request)
        
        # ë°˜ë³µ ê°œì„  ë©”íŠ¸ë¦­ ê²€ì¦
        metrics = response.metrics
        assert metrics.iterations_performed >= 1, "No iterations performed"
        assert len(metrics.quality_improvement_per_iteration) >= 1, "No quality tracking"
        
        # í’ˆì§ˆ ê°œì„  ì¶”ì´ í™•ì¸
        quality_scores = metrics.quality_improvement_per_iteration
        if len(quality_scores) > 1:
            # í’ˆì§ˆì´ ê°œì„ ë˜ê±°ë‚˜ ìµœì†Œí•œ ìœ ì§€ë˜ì–´ì•¼ í•¨
            final_quality = quality_scores[-1]
            initial_quality = quality_scores[0]
            assert final_quality >= initial_quality * 0.95, "Quality degraded significantly"
        
        print(f"âœ… Quality improved from {quality_scores[0]:.3f} to {quality_scores[-1]:.3f} in {metrics.iterations_performed} iterations")
    
    @pytest.mark.asyncio
    async def test_validation_and_auto_fix_integration(
        self,
        test_orchestrator: WorkflowOrchestrator,
        sample_business_document: str
    ):
        """ê²€ì¦ ë° ìë™ ìˆ˜ì • í†µí•© í…ŒìŠ¤íŠ¸"""
        
        request = GenerationRequest(
            document=sample_business_document,
            num_slides=6,
            target_quality_score=0.85,
            aggressive_fixing=True
        )
        
        response = await test_orchestrator.execute(request)
        
        # ê²€ì¦ ë‹¨ê³„ ê²°ê³¼ í™•ì¸
        validation_results = [
            result for result in response.stage_results 
            if result.stage == WorkflowStage.VALIDATION
        ]
        assert len(validation_results) >= 1, "No validation performed"
        
        # ìë™ ìˆ˜ì • í†µê³„ í™•ì¸
        metrics = response.metrics
        if metrics.initial_issues_count > 0:
            # ì´ìŠˆê°€ ìˆì—ˆë‹¤ë©´ ìˆ˜ì •ì´ ì‹œë„ë˜ì–´ì•¼ í•¨
            assert metrics.fixed_issues_count >= 0, "No fix attempts made"
            
            # ìˆ˜ì • ì„±ê³µë¥  í™•ì¸
            if metrics.fixed_issues_count > 0:
                assert metrics.fix_success_rate > 0, "No successful fixes"
                assert metrics.fix_success_rate <= 1.0, "Invalid fix success rate"
        
        # ìµœì¢… í’ˆì§ˆ í™•ì¸
        final_quality = response.quality_score
        assert final_quality > 0, "No final quality score"
        
        print(f"âœ… Fixed {metrics.fixed_issues_count}/{metrics.initial_issues_count} issues with {metrics.fix_success_rate:.2%} success rate")
    
    @pytest.mark.asyncio
    async def test_mckinsey_compliance_verification(
        self,
        test_orchestrator: WorkflowOrchestrator,
        sample_business_document: str
    ):
        """McKinsey í‘œì¤€ ì¤€ìˆ˜ ê²€ì¦"""
        
        request = GenerationRequest(
            document=sample_business_document,
            num_slides=8,
            template_name="mckinsey_standard",
            color_scheme="mckinsey_blue"
        )
        
        response = await test_orchestrator.execute(request)
        
        # McKinsey ì¤€ìˆ˜ ì—¬ë¶€ í™•ì¸
        assert response.mckinsey_compliance is not None, "No compliance check"
        
        # í’ˆì§ˆ ì ìˆ˜ ì„¸ë¶€ í™•ì¸
        if response.quality_breakdown:
            quality = response.quality_breakdown
            
            # ê° í’ˆì§ˆ ê¸°ì¤€ì´ í‰ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸
            assert quality.clarity >= 0, "Clarity not evaluated"
            assert quality.insight >= 0, "Insight not evaluated"
            assert quality.structure >= 0, "Structure not evaluated"
            assert quality.visual >= 0, "Visual not evaluated"
            assert quality.actionability >= 0, "Actionability not evaluated"
            
            # ê°€ì¤‘ í‰ê· ì´ ì˜¬ë°”ë¥´ê²Œ ê³„ì‚°ë˜ì—ˆëŠ”ì§€ í™•ì¸
            expected_total = (
                quality.clarity * 0.20 +
                quality.insight * 0.25 +
                quality.structure * 0.20 +
                quality.visual * 0.15 +
                quality.actionability * 0.20
            )
            assert abs(quality.total - expected_total) < 0.01, "Incorrect weighted average"
        
        print(f"âœ… McKinsey compliance: {response.mckinsey_compliance}, Quality breakdown verified")
    
    @pytest.mark.asyncio
    async def test_performance_benchmarks(
        self,
        test_orchestrator: WorkflowOrchestrator,
        sample_business_document: str
    ):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        
        # ë‹¤ì–‘í•œ ìŠ¬ë¼ì´ë“œ ìˆ˜ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        test_cases = [
            {"slides": 5, "max_time": 120},   # 2ë¶„
            {"slides": 10, "max_time": 180},  # 3ë¶„
            {"slides": 15, "max_time": 300},  # 5ë¶„
        ]
        
        for case in test_cases:
            request = GenerationRequest(
                document=sample_business_document,
                num_slides=case["slides"],
                target_quality_score=0.75,
                max_iterations=2
            )
            
            start_time = time.time()
            response = await test_orchestrator.execute(request)
            execution_time = time.time() - start_time
            
            # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
            assert execution_time <= case["max_time"], f"{case['slides']} slides took {execution_time:.1f}s (limit: {case['max_time']}s)"
            assert response.success, f"Failed with {case['slides']} slides"
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            if response.metrics:
                assert response.metrics.peak_memory_usage_mb < 1000, f"Memory usage too high: {response.metrics.peak_memory_usage_mb}MB"
            
            print(f"âœ… {case['slides']} slides: {execution_time:.1f}s (limit: {case['max_time']}s)")
    
    @pytest.mark.asyncio
    async def test_error_handling_and_recovery(
        self,
        test_orchestrator: WorkflowOrchestrator
    ):
        """ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µêµ¬ í…ŒìŠ¤íŠ¸"""
        
        # ì˜ëª»ëœ ì…ë ¥ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        invalid_requests = [
            # ë¹ˆ ë¬¸ì„œ
            GenerationRequest(document="", num_slides=5),
            # ë„ˆë¬´ ë§ì€ ìŠ¬ë¼ì´ë“œ
            GenerationRequest(document="Short doc", num_slides=100),
            # ë¶ˆê°€ëŠ¥í•œ í’ˆì§ˆ ëª©í‘œ
            GenerationRequest(document="Test document", target_quality_score=1.1)
        ]
        
        for i, request in enumerate(invalid_requests):
            response = await test_orchestrator.execute(request)
            
            # ì˜¤ë¥˜ê°€ ì ì ˆíˆ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if not response.success:
                assert len(response.errors) > 0, f"No error messages for invalid request {i}"
                assert response.pptx_path is None or not os.path.exists(response.pptx_path), f"File created for invalid request {i}"
            
            print(f"âœ… Invalid request {i+1} handled appropriately")
    
    @pytest.mark.asyncio
    async def test_concurrent_execution(
        self,
        sample_business_document: str
    ):
        """ë™ì‹œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
        
        # 3ê°œì˜ ë™ì‹œ ìš”ì²­
        requests = [
            GenerationRequest(document=sample_business_document, num_slides=6),
            GenerationRequest(document=sample_business_document, num_slides=8),
            GenerationRequest(document=sample_business_document, num_slides=10),
        ]
        
        orchestrators = [
            WorkflowOrchestrator(max_iterations=1, timeout_minutes=2)
            for _ in requests
        ]
        
        # ë™ì‹œ ì‹¤í–‰
        start_time = time.time()
        tasks = [
            orchestrator.execute(request)
            for orchestrator, request in zip(orchestrators, requests)
        ]
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        total_time = time.time() - start_time
        
        # ëª¨ë“  ìš”ì²­ì´ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
        successful_responses = []
        for i, response in enumerate(responses):
            if isinstance(response, Exception):
                print(f"âš ï¸ Request {i+1} failed with exception: {response}")
            else:
                assert hasattr(response, 'success'), f"Invalid response type for request {i+1}"
                if response.success:
                    successful_responses.append(response)
        
        # ìµœì†Œ í•˜ë‚˜ëŠ” ì„±ê³µí•´ì•¼ í•¨
        assert len(successful_responses) >= 1, "No successful concurrent executions"
        
        # ë™ì‹œ ì‹¤í–‰ì´ ìˆœì°¨ ì‹¤í–‰ë³´ë‹¤ íš¨ìœ¨ì ì´ì–´ì•¼ í•¨ (ê°ê° 2ë¶„ í•œê³„ Ã— 3 = 6ë¶„ë³´ë‹¤ ì ì–´ì•¼ í•¨)
        assert total_time < 360, f"Concurrent execution too slow: {total_time:.1f}s"
        
        print(f"âœ… {len(successful_responses)}/3 concurrent executions successful in {total_time:.1f}s")


class TestWorkflowComponentIntegration:
    """ê°œë³„ ì»´í¬ë„ŒíŠ¸ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.asyncio
    async def test_content_generator_integration(self):
        """ContentGenerator í†µí•© í…ŒìŠ¤íŠ¸"""
        from app.services.content_generator import ContentGenerator
        
        generator = ContentGenerator()
        
        test_document = "Digital transformation analysis with 25% growth potential and customer satisfaction improvement."
        
        presentation = await generator.generate(
            document=test_document,
            num_slides=5,
            target_audience="executive"
        )
        
        # ê¸°ë³¸ ê²€ì¦
        assert presentation is not None, "No presentation generated"
        assert len(presentation.slides) >= 3, "Not enough slides generated"
        assert len(presentation.slides) <= 7, "Too many slides generated"
        
        # ìŠ¬ë¼ì´ë“œ ë‚´ìš© ê²€ì¦
        for i, slide in enumerate(presentation.slides):
            assert slide.shapes.title is not None or i == 0, f"No title in slide {i+1}"
        
        print(f"âœ… ContentGenerator created {len(presentation.slides)} slides")
    
    @pytest.mark.asyncio
    async def test_design_applicator_integration(self):
        """DesignApplicator í†µí•© í…ŒìŠ¤íŠ¸"""
        from app.services.design_applicator import DesignApplicator
        from app.services.content_generator import ContentGenerator
        
        # ë¨¼ì € ê¸°ë³¸ í”„ë ˆì  í…Œì´ì…˜ ìƒì„±
        generator = ContentGenerator()
        presentation = await generator.generate(
            document="Test business analysis",
            num_slides=3
        )
        
        # ë””ìì¸ ì ìš©
        applicator = DesignApplicator()
        styled_presentation = await applicator.apply(presentation)
        
        # ì ìš© í†µê³„ í™•ì¸
        stats = applicator.get_design_stats()
        
        assert stats["stats"]["slides_processed"] > 0, "No slides processed"
        assert "mckinsey_compliance" in stats, "No compliance info"
        assert stats["mckinsey_compliance"]["color_palette"] == "McKinsey Standard", "Wrong color palette"
        
        print(f"âœ… DesignApplicator processed {stats['stats']['slides_processed']} slides")
    
    @pytest.mark.asyncio
    async def test_quality_controller_integration(self):
        """QualityController í†µí•© í…ŒìŠ¤íŠ¸"""
        from app.services.quality_controller import QualityController
        from app.services.content_generator import ContentGenerator
        from app.services.design_applicator import DesignApplicator
        
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ í”„ë ˆì  í…Œì´ì…˜ ìƒì„±
        generator = ContentGenerator()
        presentation = await generator.generate(
            document="Strategic analysis with data insights, recommendations, and next steps for business growth.",
            num_slides=4
        )
        
        applicator = DesignApplicator()
        styled_presentation = await applicator.apply(presentation)
        
        # í’ˆì§ˆ í‰ê°€
        controller = QualityController(target_score=0.8)
        quality_score = await controller.evaluate(styled_presentation)
        
        # í’ˆì§ˆ ì ìˆ˜ ê²€ì¦
        assert quality_score.total >= 0, "Invalid total score"
        assert quality_score.total <= 1.0, "Score exceeds maximum"
        
        # ê°œë³„ ê¸°ì¤€ ê²€ì¦
        assert 0 <= quality_score.clarity <= 1.0, "Invalid clarity score"
        assert 0 <= quality_score.insight <= 1.0, "Invalid insight score"
        assert 0 <= quality_score.structure <= 1.0, "Invalid structure score"
        assert 0 <= quality_score.visual <= 1.0, "Invalid visual score"
        assert 0 <= quality_score.actionability <= 1.0, "Invalid actionability score"
        
        # í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
        report = controller.get_quality_report(quality_score)
        assert "overall_score" in report, "Missing overall score in report"
        assert "grade" in report, "Missing grade in report"
        assert "improvement_suggestions" in report, "Missing suggestions in report"
        
        print(f"âœ… QualityController evaluated with score {quality_score.total:.3f} ({report['grade']})")


@pytest.mark.asyncio
async def test_demo_presentation_generation():
    """ë°ëª¨ í”„ë ˆì  í…Œì´ì…˜ ìƒì„± í…ŒìŠ¤íŠ¸"""
    
    demo_document = """
    # McKinsey ìŠ¤íƒ€ì¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ë³´ê³ ì„œ
    
    ## ê°œìš”
    ë³¸ ë¶„ì„ì€ ë””ì§€í„¸ ì „í™˜ ì´ë‹ˆì…”í‹°ë¸Œì˜ í˜„ì¬ ìƒíƒœì™€ í–¥í›„ ì „ëµ ë°©í–¥ì„ ë‹¤ë£¹ë‹ˆë‹¤.
    
    ## í˜„í™© ë¶„ì„
    - ë””ì§€í„¸ ë§¤ì¶œ ë¹„ì¤‘: í˜„ì¬ 35% â†’ ëª©í‘œ 60%
    - ê³ ê° ë””ì§€í„¸ ì±„ë„ ì´ìš©ë¥ : 67% (ì „ë…„ ëŒ€ë¹„ +23%)
    - ìš´ì˜ íš¨ìœ¨ì„±: ìë™í™”ë¥¼ í†µí•œ 15% ë¹„ìš© ì ˆê° ë‹¬ì„±
    
    ## í•µì‹¬ ì¸ì‚¬ì´íŠ¸
    1. ê³ ê° í–‰ë™ ë³€í™”: ëª¨ë°”ì¼ ìš°ì„  ì „ëµ í•„ìš”
    2. ê²½ìŸ í™˜ê²½: ë””ì§€í„¸ ë„¤ì´í‹°ë¸Œ ê¸°ì—…ë“¤ì˜ ì‹œì¥ ì ìœ ìœ¨ í™•ëŒ€
    3. ë‚´ë¶€ ì—­ëŸ‰: ë””ì§€í„¸ ìŠ¤í‚¬ ê°­ ì¡´ì¬, êµìœ¡ íˆ¬ì í•„ìš”
    
    ## ì „ëµì  ê¶Œê³ ì‚¬í•­
    1. ë””ì§€í„¸ í”Œë«í¼ íˆ¬ì í™•ëŒ€ (ìš°ì„ ìˆœìœ„: ëª¨ë°”ì¼ ì•±, AI ì±—ë´‡)
    2. ì¡°ì§ ì—­ëŸ‰ ê°•í™” (ë””ì§€í„¸ êµìœ¡ í”„ë¡œê·¸ë¨, ì• ìì¼ ì¡°ì§ ì „í™˜)
    3. ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì²´ê³„ êµ¬ì¶•
    
    ## ì‹¤í–‰ ê³„íš
    - Phase 1 (0-3ê°œì›”): ëª¨ë°”ì¼ í”Œë«í¼ ê°œì„ , êµìœ¡ í”„ë¡œê·¸ë¨ ì‹œì‘
    - Phase 2 (3-6ê°œì›”): AI ì†”ë£¨ì…˜ ë„ì…, ì¡°ì§ êµ¬ì¡° ê°œí¸
    - Phase 3 (6-12ê°œì›”): ë°ì´í„° ë¶„ì„ í”Œë«í¼ êµ¬ì¶•, ì„±ê³¼ ì¸¡ì •
    
    ## ê¸°ëŒ€ íš¨ê³¼
    - ë§¤ì¶œ ì¦ê°€: 25-30% (12ê°œì›” ë‚´)
    - ê³ ê° ë§Œì¡±ë„: í˜„ì¬ 78% â†’ ëª©í‘œ 90%
    - ìš´ì˜ ë¹„ìš© ì ˆê°: ì¶”ê°€ 10-15%
    """
    
    orchestrator = WorkflowOrchestrator(
        max_iterations=3,
        target_quality_score=0.85,
        aggressive_fix=True
    )
    
    request = GenerationRequest(
        document=demo_document,
        num_slides=12,
        target_audience="executive",
        presentation_purpose="analysis",
        target_quality_score=0.85,
        include_charts=True,
        include_recommendations=True
    )
    
    print("ğŸš€ Generating demo McKinsey-style presentation...")
    
    start_time = time.time()
    response = await orchestrator.execute(request)
    execution_time = time.time() - start_time
    
    # ê²°ê³¼ ê²€ì¦ ë° ì¶œë ¥
    print(f"\nğŸ“Š Demo Presentation Results:")
    print(f"Success: {response.success}")
    print(f"Execution Time: {execution_time:.1f}s")
    print(f"Slides Generated: {response.slides_generated}")
    print(f"Quality Score: {response.quality_score:.3f}")
    print(f"McKinsey Compliant: {response.mckinsey_compliance}")
    
    if response.pptx_path and os.path.exists(response.pptx_path):
        file_size = os.path.getsize(response.pptx_path) / 1024
        print(f"Output File: {response.pptx_path} ({file_size:.1f}KB)")
    
    if response.metrics:
        print(f"Iterations: {response.metrics.iterations_performed}")
        print(f"Issues Fixed: {response.metrics.fixed_issues_count}")
        print(f"Peak Memory: {response.metrics.peak_memory_usage_mb:.1f}MB")
    
    if response.errors:
        print(f"Errors: {len(response.errors)}")
        for error in response.errors[:3]:  # ì²« 3ê°œë§Œ í‘œì‹œ
            print(f"  - {error}")
    
    # í’ˆì§ˆ ì„¸ë¶€ ì ìˆ˜
    if response.quality_breakdown:
        q = response.quality_breakdown
        print(f"\nğŸ“ˆ Quality Breakdown:")
        print(f"  Clarity: {q.clarity:.3f} (20%)")
        print(f"  Insight: {q.insight:.3f} (25%)")
        print(f"  Structure: {q.structure:.3f} (20%)")
        print(f"  Visual: {q.visual:.3f} (15%)")
        print(f"  Actionability: {q.actionability:.3f} (20%)")
    
    # í…ŒìŠ¤íŠ¸ ì–´ì„¤ì…˜
    assert response.success, f"Demo generation failed: {response.errors}"
    assert execution_time <= 300, f"Demo generation too slow: {execution_time}s"
    assert response.quality_score >= 0.70, f"Demo quality too low: {response.quality_score}"
    
    print(f"\nâœ… Demo presentation successfully generated and validated!")
    
    return response


if __name__ == "__main__":
    # ë°ëª¨ ì‹¤í–‰
    asyncio.run(test_demo_presentation_generation())