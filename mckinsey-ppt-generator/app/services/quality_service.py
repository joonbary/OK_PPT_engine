"""
Quality Service - Fix #4
í’ˆì§ˆ ê²€ì¦ ë° ìë™ ìˆ˜ì • ì„œë¹„ìŠ¤
"""

from typing import Dict, Any, List, Optional
from pptx import Presentation
import logging
from app.core.quality_validator import McKinseyQualityValidator

logger = logging.getLogger(__name__)


class QualityService:
    """í’ˆì§ˆ ê²€ì¦ ë° ìë™ ìˆ˜ì • ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.validator = McKinseyQualityValidator()
        self.quality_threshold = 0.85
        self.max_iterations = 3
    
    async def validate_and_improve_presentation(
        self, 
        prs: Presentation, 
        auto_fix: bool = True,
        target_quality: float = None
    ) -> Dict[str, Any]:
        """
        í”„ë ˆì  í…Œì´ì…˜ í’ˆì§ˆ ê²€ì¦ ë° ê°œì„ 
        
        Args:
            prs: PowerPoint í”„ë ˆì  í…Œì´ì…˜
            auto_fix: ìë™ ìˆ˜ì • ì—¬ë¶€
            target_quality: ëª©í‘œ í’ˆì§ˆ ì ìˆ˜
            
        Returns:
            ê²€ì¦ ë° ê°œì„  ê²°ê³¼
        """
        if target_quality is None:
            target_quality = self.quality_threshold
        
        logger.info(f"ğŸ” í’ˆì§ˆ ê²€ì¦ ì‹œì‘ (ëª©í‘œ: {target_quality:.2f})")
        
        all_results = []
        iteration = 0
        current_quality = 0.0
        
        while iteration < self.max_iterations:
            iteration += 1
            logger.info(f"ğŸ“‹ í’ˆì§ˆ ê²€ì¦ ë°˜ë³µ {iteration}/{self.max_iterations}")
            
            # í’ˆì§ˆ ê²€ì¦ ë° ìë™ ìˆ˜ì •
            result = self.validator.validate_presentation(prs, auto_fix=auto_fix)
            current_quality = result['quality_score']
            
            all_results.append({
                'iteration': iteration,
                'quality_score': current_quality,
                'issues_found': result['total_issues'],
                'fixes_applied': result['total_fixes'],
                'passed': result['passed']
            })
            
            logger.info(f"ë°˜ë³µ {iteration} ê²°ê³¼: í’ˆì§ˆ {current_quality:.3f}, ì´ìŠˆ {result['total_issues']}ê°œ, ìˆ˜ì • {result['total_fixes']}ê°œ")
            
            # ëª©í‘œ í’ˆì§ˆì— ë„ë‹¬í•˜ê±°ë‚˜ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¤‘ë‹¨
            if current_quality >= target_quality:
                logger.info(f"âœ… ëª©í‘œ í’ˆì§ˆ ë‹¬ì„±: {current_quality:.3f} >= {target_quality:.2f}")
                break
            
            if result['total_fixes'] == 0:
                logger.info("â„¹ï¸ ë” ì´ìƒ ìë™ ìˆ˜ì •í•  ë‚´ìš©ì´ ì—†ìŒ")
                break
        
        # ìµœì¢… ê²°ê³¼
        final_result = {
            'final_quality_score': current_quality,
            'target_quality': target_quality,
            'quality_achieved': current_quality >= target_quality,
            'total_iterations': iteration,
            'iteration_results': all_results,
            'total_issues_resolved': sum(r['fixes_applied'] for r in all_results),
            'improvement': all_results[-1]['quality_score'] - all_results[0]['quality_score'] if all_results else 0
        }
        
        # ìµœì¢… ìƒì„¸ ê²€ì¦
        final_validation = self.validator.validate_presentation(prs, auto_fix=False)
        final_result['final_validation'] = final_validation
        
        logger.info(f"ğŸ¯ ìµœì¢… í’ˆì§ˆ: {current_quality:.3f}, ê°œì„ ë„: {final_result['improvement']:.3f}")
        
        return final_result
    
    def generate_improvement_recommendations(self, validation_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        issues = validation_result.get('issues', {})
        
        # ìŠ¤íƒ€ì¼ ìœ„ë°˜ ê¶Œì¥ì‚¬í•­
        style_violations = issues.get('style_violations', [])
        if style_violations:
            font_issues = [v for v in style_violations if v['type'] == 'wrong_font']
            color_issues = [v for v in style_violations if v['type'] == 'wrong_color']
            
            if font_issues:
                recommendations.append({
                    'category': 'style',
                    'priority': 'high',
                    'title': 'í°íŠ¸ í‘œì¤€í™”',
                    'description': f'{len(font_issues)}ê°œ ìŠ¬ë¼ì´ë“œì—ì„œ ë¹„í‘œì¤€ í°íŠ¸ ì‚¬ìš©ë¨',
                    'action': 'Arial í°íŠ¸ë¡œ í†µì¼',
                    'automated': True
                })
            
            if color_issues:
                recommendations.append({
                    'category': 'style',
                    'priority': 'medium',
                    'title': 'ìƒ‰ìƒ í‘œì¤€í™”',
                    'description': f'{len(color_issues)}ê°œ ìš”ì†Œì—ì„œ McKinsey ìƒ‰ìƒ ê°€ì´ë“œ ìœ„ë°˜',
                    'action': 'McKinsey ë¸Œëœë“œ ìƒ‰ìƒìœ¼ë¡œ ìˆ˜ì •',
                    'automated': True
                })
        
        # ë ˆì´ì•„ì›ƒ ë¬¸ì œ ê¶Œì¥ì‚¬í•­
        layout_issues = issues.get('layout_issues', [])
        if layout_issues:
            missing_titles = [i for i in layout_issues if i['type'] == 'missing_title']
            
            if missing_titles:
                recommendations.append({
                    'category': 'layout',
                    'priority': 'high',
                    'title': 'ì œëª© ì¶”ê°€',
                    'description': f'{len(missing_titles)}ê°œ ìŠ¬ë¼ì´ë“œì— ì œëª© ëˆ„ë½',
                    'action': 'ê° ìŠ¬ë¼ì´ë“œì— ëª…í™•í•œ ì œëª© ì¶”ê°€',
                    'automated': False
                })
        
        # í…ìŠ¤íŠ¸ ë¬¸ì œ ê¶Œì¥ì‚¬í•­
        text_problems = issues.get('text_problems', [])
        if text_problems:
            overflow_issues = [p for p in text_problems if 'overflow' in p['type']]
            
            if overflow_issues:
                recommendations.append({
                    'category': 'layout',
                    'priority': 'high',
                    'title': 'í…ìŠ¤íŠ¸ ì˜¤ë²„í”Œë¡œìš° ìˆ˜ì •',
                    'description': f'{len(overflow_issues)}ê°œ ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ê°€ ìŠ¬ë¼ì´ë“œ ê²½ê³„ ì´ˆê³¼',
                    'action': 'í…ìŠ¤íŠ¸ ë°•ìŠ¤ í¬ê¸° ì¡°ì • ë˜ëŠ” ë‚´ìš© ë‹¨ì¶•',
                    'automated': True
                })
        
        # ì°¨íŠ¸ ë¬¸ì œ ê¶Œì¥ì‚¬í•­
        chart_errors = issues.get('chart_errors', [])
        if chart_errors:
            small_charts = [e for e in chart_errors if e['type'] == 'chart_too_small']
            
            if small_charts:
                recommendations.append({
                    'category': 'visualization',
                    'priority': 'medium',
                    'title': 'ì°¨íŠ¸ í¬ê¸° ìµœì í™”',
                    'description': f'{len(small_charts)}ê°œ ì°¨íŠ¸ê°€ ë„ˆë¬´ ì‘ìŒ',
                    'action': 'ì°¨íŠ¸ í¬ê¸°ë¥¼ ê°€ë…ì„±ì„ ìœ„í•´ í™•ëŒ€',
                    'automated': True
                })
        
        # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
        priority_order = {'high': 0, 'medium': 1, 'low': 2}
        recommendations.sort(key=lambda x: priority_order.get(x['priority'], 3))
        
        return recommendations
    
    def create_quality_dashboard(self, validation_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±"""
        if not validation_results:
            return {}
        
        latest = validation_results[-1]
        
        # í’ˆì§ˆ íŠ¸ë Œë“œ
        quality_trend = [r['quality_score'] for r in validation_results]
        
        # ì´ìŠˆ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
        issues = latest.get('final_validation', {}).get('issues', {})
        issue_distribution = {
            category: len(problems) 
            for category, problems in issues.items()
        }
        
        # ê°œì„  ë©”íŠ¸ë¦­
        if len(validation_results) > 1:
            improvement_rate = (
                latest['quality_score'] - validation_results[0]['quality_score']
            ) / validation_results[0]['quality_score'] * 100
        else:
            improvement_rate = 0
        
        dashboard = {
            'current_quality': latest['quality_score'],
            'quality_trend': quality_trend,
            'issue_distribution': issue_distribution,
            'total_iterations': len(validation_results),
            'improvement_rate': improvement_rate,
            'recommendations': self.generate_improvement_recommendations(
                latest.get('final_validation', {})
            ),
            'quality_status': self._get_quality_status(latest['quality_score']),
            'automated_fixes_available': any(
                r.get('automated', False) 
                for r in self.generate_improvement_recommendations(
                    latest.get('final_validation', {})
                )
            )
        }
        
        return dashboard
    
    def _get_quality_status(self, quality_score: float) -> Dict[str, Any]:
        """í’ˆì§ˆ ìƒíƒœ ë¶„ì„"""
        if quality_score >= 0.9:
            status = 'excellent'
            message = 'íƒì›”í•œ í’ˆì§ˆ'
            color = 'green'
        elif quality_score >= 0.8:
            status = 'good'
            message = 'ì–‘í˜¸í•œ í’ˆì§ˆ'
            color = 'blue'
        elif quality_score >= 0.7:
            status = 'acceptable'
            message = 'ìˆ˜ìš© ê°€ëŠ¥í•œ í’ˆì§ˆ'
            color = 'yellow'
        elif quality_score >= 0.6:
            status = 'needs_improvement'
            message = 'ê°œì„  í•„ìš”'
            color = 'orange'
        else:
            status = 'poor'
            message = 'í’ˆì§ˆ ë¯¸ë‹¬'
            color = 'red'
        
        return {
            'status': status,
            'message': message,
            'color': color,
            'score': quality_score
        }
    
    async def batch_quality_check(self, presentations: List[Presentation]) -> Dict[str, Any]:
        """ì—¬ëŸ¬ í”„ë ˆì  í…Œì´ì…˜ ì¼ê´„ í’ˆì§ˆ ê²€ì¦"""
        logger.info(f"ğŸ“Š ì¼ê´„ í’ˆì§ˆ ê²€ì¦ ì‹œì‘: {len(presentations)}ê°œ í”„ë ˆì  í…Œì´ì…˜")
        
        results = []
        total_issues = 0
        total_fixes = 0
        
        for i, prs in enumerate(presentations):
            logger.info(f"ê²€ì¦ ì¤‘: {i+1}/{len(presentations)}")
            
            result = await self.validate_and_improve_presentation(prs)
            results.append({
                'presentation_index': i,
                'quality_score': result['final_quality_score'],
                'issues_resolved': result['total_issues_resolved'],
                'iterations': result['total_iterations']
            })
            
            total_issues += result['total_issues_resolved']
            total_fixes += result['total_issues_resolved']
        
        # ì „ì²´ í†µê³„
        avg_quality = sum(r['quality_score'] for r in results) / len(results)
        quality_distribution = {
            'excellent': len([r for r in results if r['quality_score'] >= 0.9]),
            'good': len([r for r in results if 0.8 <= r['quality_score'] < 0.9]),
            'acceptable': len([r for r in results if 0.7 <= r['quality_score'] < 0.8]),
            'needs_improvement': len([r for r in results if r['quality_score'] < 0.7])
        }
        
        batch_result = {
            'total_presentations': len(presentations),
            'average_quality': avg_quality,
            'quality_distribution': quality_distribution,
            'total_issues_resolved': total_issues,
            'individual_results': results,
            'processing_summary': {
                'presentations_improved': len([r for r in results if r['issues_resolved'] > 0]),
                'total_iterations': sum(r['iterations'] for r in results),
                'avg_iterations_per_presentation': sum(r['iterations'] for r in results) / len(results)
            }
        }
        
        logger.info(f"âœ… ì¼ê´„ ê²€ì¦ ì™„ë£Œ: í‰ê·  í’ˆì§ˆ {avg_quality:.3f}, ì´ {total_issues}ê°œ ì´ìŠˆ í•´ê²°")
        
        return batch_result