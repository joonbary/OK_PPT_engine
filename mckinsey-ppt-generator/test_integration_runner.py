#!/usr/bin/env python3
"""
Integration Test Runner for Task 4.1 Complete Workflow
Direct execution without pytest dependencies
"""

import asyncio
import sys
import os
import time
import traceback
from pathlib import Path

# Add app directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))

from app.services.workflow_orchestrator import WorkflowOrchestrator
from app.models.workflow_models import GenerationRequest


async def test_basic_workflow_integration():
    """ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ§ª Starting Basic Workflow Integration Test...")
    
    sample_document = """
    # ë””ì§€í„¸ ì „í™˜ ì „ëµ ë¶„ì„
    
    ## í˜„í™©
    - ë””ì§€í„¸ ë§¤ì¶œ ë¹„ì¤‘: 35%
    - ê³ ê° ë§Œì¡±ë„: 78%
    - ìš´ì˜ íš¨ìœ¨ì„±: ìë™í™”ë¥¼ í†µí•œ 15% ë¹„ìš© ì ˆê°
    
    ## ì£¼ìš” ë°œê²¬ì‚¬í•­
    1. ëª¨ë°”ì¼ ìš°ì„  ê³ ê° í–‰ë™ ë³€í™”
    2. ê²½ìŸì‚¬ ë””ì§€í„¸ í˜ì‹  ê°€ì†í™”
    3. ë‚´ë¶€ ë””ì§€í„¸ ìŠ¤í‚¬ ê°­ ì¡´ì¬
    
    ## ì „ëµ ê¶Œê³ ì‚¬í•­
    1. ë””ì§€í„¸ í”Œë«í¼ íˆ¬ì í™•ëŒ€
    2. ì¡°ì§ ì—­ëŸ‰ ê°•í™”
    3. ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì²´ê³„ êµ¬ì¶•
    
    ## ê¸°ëŒ€ íš¨ê³¼
    - ë§¤ì¶œ 25% ì¦ê°€
    - ê³ ê° ë§Œì¡±ë„ 90% ë‹¬ì„±
    - ìš´ì˜ ë¹„ìš© 10% ì¶”ê°€ ì ˆê°
    """
    
    try:
        # ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”
        orchestrator = WorkflowOrchestrator(
            max_iterations=2,
            target_quality_score=0.80,
            aggressive_fix=True,
            timeout_minutes=3
        )
        
        # ìƒì„± ìš”ì²­ ì„¤ì •
        request = GenerationRequest(
            document=sample_document,
            num_slides=8,
            target_audience="executive",
            presentation_purpose="analysis",
            target_quality_score=0.80,
            max_iterations=2
        )
        
        print(f"ğŸ“‹ Request: {request.num_slides} slides, target quality {request.target_quality_score}")
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        start_time = time.time()
        response = await orchestrator.execute(request)
        execution_time = time.time() - start_time
        
        # ê²°ê³¼ ê²€ì¦
        print(f"\nğŸ“Š Results:")
        print(f"âœ… Success: {response.success}")
        print(f"â±ï¸  Execution Time: {execution_time:.1f}s")
        print(f"ğŸ“„ Slides Generated: {response.slides_generated}")
        print(f"ğŸ¯ Quality Score: {response.quality_score:.3f}")
        print(f"ğŸ¢ McKinsey Compliant: {response.mckinsey_compliance}")
        
        if response.pptx_path and os.path.exists(response.pptx_path):
            file_size = os.path.getsize(response.pptx_path) / 1024
            print(f"ğŸ’¾ Output File: {response.pptx_path} ({file_size:.1f}KB)")
        else:
            print(f"âš ï¸  No output file generated")
        
        if response.metrics:
            print(f"ğŸ”„ Iterations: {response.metrics.iterations_performed}")
            print(f"ğŸ”§ Issues Fixed: {response.metrics.fixed_issues_count}")
            print(f"ğŸ’¾ Peak Memory: {response.metrics.peak_memory_usage_mb:.1f}MB")
        
        # ì—ëŸ¬ ì¶œë ¥
        if response.errors:
            print(f"âŒ Errors ({len(response.errors)}):")
            for error in response.errors[:3]:
                print(f"   - {error}")
        
        # í’ˆì§ˆ ì„¸ë¶€ ì ìˆ˜
        if response.quality_breakdown:
            q = response.quality_breakdown
            print(f"\nğŸ“ˆ Quality Breakdown:")
            print(f"   Clarity: {q.clarity:.3f} (20%)")
            print(f"   Insight: {q.insight:.3f} (25%)")
            print(f"   Structure: {q.structure:.3f} (20%)")
            print(f"   Visual: {q.visual:.3f} (15%)")
            print(f"   Actionability: {q.actionability:.3f} (20%)")
        
        # í…ŒìŠ¤íŠ¸ ì–´ì„¤ì…˜
        assert response.success, f"Workflow failed: {response.errors}"
        assert execution_time <= 180, f"Too slow: {execution_time}s"
        assert response.quality_score >= 0.60, f"Quality too low: {response.quality_score}"
        assert response.slides_generated >= 5, f"Not enough slides: {response.slides_generated}"
        
        print(f"\nâœ… Basic Workflow Integration Test PASSED!")
        return True
        
    except Exception as e:
        print(f"\nâŒ Basic Workflow Integration Test FAILED: {e}")
        print(f"   Traceback: {traceback.format_exc()}")
        return False


async def test_component_integration():
    """ê°œë³„ ì»´í¬ë„ŒíŠ¸ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    print("\nğŸ§ª Starting Component Integration Test...")
    
    try:
        # ContentGenerator í…ŒìŠ¤íŠ¸
        print("ğŸ¯ Testing ContentGenerator...")
        from app.services.content_generator import ContentGenerator
        
        generator = ContentGenerator()
        presentation = await generator.generate(
            document="Test business analysis with recommendations",
            num_slides=5,
            target_audience="executive"
        )
        
        assert presentation is not None, "ContentGenerator failed to create presentation"
        assert len(presentation.slides) >= 3, f"Too few slides: {len(presentation.slides)}"
        print(f"   âœ… Generated {len(presentation.slides)} slides")
        
        # DesignApplicator í…ŒìŠ¤íŠ¸
        print("ğŸ¨ Testing DesignApplicator...")
        from app.services.design_applicator import DesignApplicator
        
        applicator = DesignApplicator()
        styled_presentation = await applicator.apply(presentation)
        
        stats = applicator.get_design_stats()
        assert stats["stats"]["slides_processed"] > 0, "No slides processed by DesignApplicator"
        print(f"   âœ… Styled {stats['stats']['slides_processed']} slides")
        
        # QualityController í…ŒìŠ¤íŠ¸
        print("ğŸ“Š Testing QualityController...")
        from app.services.quality_controller import QualityController
        
        controller = QualityController(target_score=0.8)
        quality_score = await controller.evaluate(styled_presentation)
        
        assert 0 <= quality_score.total <= 1.0, f"Invalid quality score: {quality_score.total}"
        print(f"   âœ… Quality evaluated: {quality_score.total:.3f}")
        
        print(f"\nâœ… Component Integration Test PASSED!")
        return True
        
    except Exception as e:
        print(f"\nâŒ Component Integration Test FAILED: {e}")
        print(f"   Traceback: {traceback.format_exc()}")
        return False


async def test_performance_benchmark():
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    
    print("\nğŸ§ª Starting Performance Benchmark Test...")
    
    try:
        test_cases = [
            {"slides": 5, "max_time": 90},
            {"slides": 10, "max_time": 150},
        ]
        
        document = "Business analysis with growth strategy, customer insights, and operational recommendations."
        
        for case in test_cases:
            print(f"â±ï¸  Testing {case['slides']} slides (limit: {case['max_time']}s)...")
            
            orchestrator = WorkflowOrchestrator(
                max_iterations=2,
                target_quality_score=0.75,
                timeout_minutes=3
            )
            
            request = GenerationRequest(
                document=document,
                num_slides=case["slides"],
                target_quality_score=0.75,
                max_iterations=2
            )
            
            start_time = time.time()
            response = await orchestrator.execute(request)
            execution_time = time.time() - start_time
            
            print(f"   â±ï¸  Completed in {execution_time:.1f}s")
            print(f"   ğŸ¯ Quality: {response.quality_score:.3f}")
            
            # ì„±ëŠ¥ ê²€ì¦
            if execution_time > case["max_time"]:
                print(f"   âš ï¸  Warning: Exceeded time limit ({execution_time:.1f}s > {case['max_time']}s)")
            else:
                print(f"   âœ… Within time limit")
            
            assert response.success or len(response.errors) > 0, "No response or errors"
        
        print(f"\nâœ… Performance Benchmark Test COMPLETED!")
        return True
        
    except Exception as e:
        print(f"\nâŒ Performance Benchmark Test FAILED: {e}")
        print(f"   Traceback: {traceback.format_exc()}")
        return False


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    
    print("Starting Task 4.1 Complete Workflow Integration Tests")
    print("=" * 70)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path("output/generated_presentations")
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“ Output directory: {output_dir}")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_results = []
    
    # 1. ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸
    result1 = await test_basic_workflow_integration()
    test_results.append(("Basic Workflow Integration", result1))
    
    # 2. ì»´í¬ë„ŒíŠ¸ í†µí•© í…ŒìŠ¤íŠ¸
    result2 = await test_component_integration()
    test_results.append(("Component Integration", result2))
    
    # 3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
    result3 = await test_performance_benchmark()
    test_results.append(("Performance Benchmark", result3))
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 70)
    print("ğŸ“‹ Test Results Summary:")
    print("=" * 70)
    
    passed = 0
    total = len(test_results)
    
    for test_name, passed_test in test_results:
        status = "âœ… PASSED" if passed_test else "âŒ FAILED"
        print(f"   {test_name}: {status}")
        if passed_test:
            passed += 1
    
    print(f"\nğŸ¯ Overall: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("ğŸ‰ ALL TESTS PASSED! Task 4.1 Integration Complete!")
        return True
    else:
        print("âš ï¸  Some tests failed. Review the errors above.")
        return False


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâš ï¸  Tests interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ Unexpected error: {e}")
        print(f"   Traceback: {traceback.format_exc()}")
        sys.exit(1)